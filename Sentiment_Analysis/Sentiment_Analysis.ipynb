{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emojify! \n",
    "\n",
    "In this notebook i will implement an important application of NLP Sentiment Analysis,Our goal is determine the sentiment of a sentence for example does the sentence repersent happiness or sadness ? to make it more modern we are going to experess the sentence sentiment with an emoijy (Emojifier). \n",
    "\n",
    "* The Emojifier app will implement a model which inputs a sentence (such as \"Let's go see the baseball game tonight!\") and finds the most appropriate emoji to be used with this sentence (‚öæÔ∏è)\n",
    "So rather than writing:\n",
    ">\"Congratulations on the promotion! Let's get coffee and talk. Love you!\"   \n",
    "\n",
    "The emojifier can automatically turn this into:\n",
    ">\"Congratulations on the promotion! üëç Let's get coffee and talk. ‚òïÔ∏è Love you! ‚ù§Ô∏è\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from emo_utils import *\n",
    "import emoji\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Dataset EMOJISET\n",
    "\n",
    "We have a tiny dataset (X, Y) where:\n",
    "- X contains 127 sentences (strings).\n",
    "- Y contains an integer label between 0 and 4 corresponding to an emoji for each sentence.\n",
    "\n",
    "<img src=\"images/data_set.png\" style=\"width:700px;height:300px;\">\n",
    "<caption><center> **Figure 1**: EMOJISET - a classification problem with 5 classes. A few examples of sentences are given here. </center></caption>\n",
    "\n",
    "Let's load the dataset using the code below. We split the dataset between training (127 examples) and testing (56 examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = read_csv('data/train_emoji.csv')\n",
    "X_test, Y_test = read_csv('data/tesss.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxLen = len(max(X_train, key=len).split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentences from X_train and corresponding labels from Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "never talk to me again üòû\n",
      "I am proud of your achievements üòÑ\n",
      "It is the worst day in my life üòû\n",
      "Miss you so much ‚ù§Ô∏è\n",
      "food is life üç¥\n",
      "I love you mum ‚ù§Ô∏è\n",
      "Stop saying bullshit üòû\n",
      "congratulations on your acceptance üòÑ\n",
      "The assignment is too long  üòû\n",
      "I want to go play ‚öæ\n"
     ]
    }
   ],
   "source": [
    "for idx in range(10):\n",
    "    print(X_train[idx], label_to_emoji(Y_train[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emojifier Using LSTMs in Keras: \n",
    "\n",
    "Let's build an LSTM model that takes word **sequences** as input!\n",
    "\n",
    "* Emojifier will use pre-trained word embeddings to represent words.\n",
    "* We will feed word embeddings into an LSTM.\n",
    "* The LSTM will learn to predict the most appropriate emoji. \n",
    "* This model will be able to account for the word ordering. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Emmbeddings is Vectors used to reperesent what the words mean and their correlation with each other for example: if we input a sentence man is to woman king is to ?? , the Emmbedding vector should learn that the answer is queen so the vectors measure some features for each word for example word man it learns features like these is it male -> 1, is it female -> 0 is it food -> 0 and so on \n",
    "\n",
    "We use these vectors to learn meanings of words in the sentences inputted to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Overview of the model\n",
    "\n",
    "Here is the Model we will implement:\n",
    "\n",
    "<img src=\"images/emojifier-v2.png\" style=\"width:700px;height:400px;\"> <br>\n",
    "<caption><center> **Figure 2**: Emojifier. A 2-layer LSTM sequence classifier. </center></caption>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Network Structure\n",
    "* The `Embedding()` layer's input is an integer matrix of size **(batch size, max input length)**. \n",
    "    * This Layer computes Word Emmbeddings for input sentence\n",
    "    * This input corresponds to sentences converted into lists of indices (integers). so we will build a function to map sentences to indices\n",
    "\n",
    "* These Computed Word Emmbeddings will be pushd into LSTM (Long Short Term Memoery) Reccurent neural network to make a prediction about what emoji reperesent the sentence\n",
    "\n",
    "* Dropout Regularization will be used to reduce Over-fiiting\n",
    "\n",
    "* The Output will fed again to a Series of LSTMs and then We will apply Regularization again and finally softmax output to make the final prediction \n",
    "\n",
    "We will implement the above model in keras using mini-batches to train the model \n",
    "   * Keras require that all sequences in the same mini-batch have the **same length**\n",
    "   * The common solution to handling sequences of **different length** is to use padding.  Specifically:\n",
    "   * Set a maximum sequence length\n",
    "   * Pad all sequences to have the same length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Emmbedding Vectors is expensive and requries alot of time so in the next cell i will load pre-trained Emmbedding Vectors and use it to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file,'r', encoding=\"utf8\") as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "        \n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words_to_index, index_to_words, word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('data\\\\glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences_to_indices\n",
    "\n",
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    \"\"\"\n",
    "    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n",
    "    The output shape should be such that it can be given to `Embedding()` (described in Figure 4). \n",
    "    \n",
    "    Arguments:\n",
    "    X -- array of sentences (strings), of shape (m, 1)\n",
    "    word_to_index -- a dictionary containing the each word mapped to its index\n",
    "    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. \n",
    "    \n",
    "    Returns:\n",
    "    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0]                                   # number of training examples\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Initialize X_indices as a numpy matrix of zeros and the correct shape (‚âà 1 line)\n",
    "    X_indices = np.zeros([m,max_len])\n",
    "    \n",
    "    for i in range(m):                               # loop over training examples\n",
    "        \n",
    "        # Convert the ith training sentence in lower case and split is into words. You should get a list of words.\n",
    "        sentence_words = [w.lower() for w in X[i].split()] \n",
    "        \n",
    "        # Initialize j to 0\n",
    "        j = 0\n",
    "        \n",
    "        # Loop over the words of sentence_words\n",
    "        for w in sentence_words:\n",
    "            # Set the (i,j)th entry of X_indices to the index of the correct word.\n",
    "            X_indices[i, j] = word_to_index[w]\n",
    "            # Increment j to j + 1\n",
    "            j += 1\n",
    "            \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1 = ['funny lol' 'lets play baseball' 'food is ready for you']\n",
      "X1_indices =\n",
      " [[155345. 225122.      0.      0.      0.]\n",
      " [220930. 286375.  69714.      0.      0.]\n",
      " [151204. 192973. 302254. 151349. 394475.]]\n"
     ]
    }
   ],
   "source": [
    "X1 = np.array([\"funny lol\", \"lets play baseball\", \"food is ready for you\"])\n",
    "X1_indices = sentences_to_indices(X1,word_to_index, max_len = 5)\n",
    "print(\"X1 =\", X1)\n",
    "print(\"X1_indices =\\n\", X1_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build embedding layer\n",
    "\n",
    "* Let's build the `Embedding()` layer in Keras, using pre-trained word vectors. \n",
    "* The embedding layer takes as input a list of word indices.\n",
    "    * `sentences_to_indices()` creates these word indices.\n",
    "* The embedding layer will return the word embeddings for a sentence. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n",
    "    \n",
    "    Arguments:\n",
    "    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    embedding_layer -- pretrained layer Keras instance\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)\n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      # define dimensionality of your GloVe word vectors (= 50)\n",
    "    \n",
    "    \n",
    "    emb_matrix = np.zeros([vocab_len,emb_dim])\n",
    "    \n",
    "    # Set each row \"idx\" of the embedding matrix to be \n",
    "    # the word vector representation of the idx'th word of the vocabulary\n",
    "    for word, idx in word_to_index.items():\n",
    "        emb_matrix[idx, :] = word_to_vec_map[word]\n",
    "\n",
    "    # Define Keras embedding layer with the correct input and output sizes\n",
    "    # non-trainable.\n",
    "    embedding_layer = Embedding(\n",
    "    vocab_len,\n",
    "    emb_dim,\n",
    "    trainable = False)\n",
    "\n",
    "    # Build the embedding layer. \n",
    "    embedding_layer.build((None,)) \n",
    "    \n",
    "    # Set the weights of the embedding layer to the embedding matrix.\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights[0][1][3] = -0.3403\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "print(\"weights[0][1][3] =\", embedding_layer.get_weights()[0][1][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Emojifier\n",
    "\n",
    "Lets now build the Emojifier model we previouly showed in figure 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Emojify(input_shape, word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Function creating the Emojify model's graph.\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of the input, usually (max_len,)\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    model -- a model instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define sentence_indices as the input of the graph.\n",
    "    sentence_indices = Input(shape=input_shape, dtype='int32')\n",
    "    \n",
    "    # Create the embedding layer pretrained with GloVe Vectors \n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    \n",
    "    # Propagate sentence_indices through your embedding layer\n",
    "    embeddings = embedding_layer(sentence_indices)  \n",
    "    \n",
    "    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n",
    "    # The returned output is a batch of sequences.\n",
    "    X = LSTM(units = 128, return_sequences= True)(embeddings)\n",
    "    # Add dropout \n",
    "    X = Dropout(rate = 0.5)(X)\n",
    "    # Propagate X trough another LSTM layer with 128-dimensional hidden state\n",
    "    # The returned output is a single hidden state\n",
    "    X = LSTM(units = 128, return_sequences= False)(X)\n",
    "    # Add dropout \n",
    "    X = Dropout(rate = 0.5)(X)\n",
    "    # Propagate X through a Dense layer with 5 units\n",
    "    X = Dense(units = 5,activation='softmax')(X)\n",
    "    # Add a softmax activation\n",
    "    X = Activation('softmax')(X)\n",
    "    \n",
    "    # Create Model instance which converts sentence_indices into X.\n",
    "    model = Model(inputs= sentence_indices, outputs= X)\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, 10, 50)            20000050  \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 10, 128)           91648     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 10, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 645       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 20,223,927\n",
      "Trainable params: 223,877\n",
      "Non-trainable params: 20,000,050\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Emojify((maxLen,), word_to_vec_map, word_to_index)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will Compile our model using `categorical_crossentropy` loss, `adam` optimizer and `['accuracy']` metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to train our model. Emojifier `model` takes as input an array of shape (`m`, `max_len`) and outputs probability vectors of shape (`m`, `number of classes`). We thus have to convert X_train (array of sentences as strings) to X_train_indices (array of sentences as list of word indices), and Y_train (labels as indices) to Y_train_oh (labels as one-hot vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)\n",
    "Y_train_oh = convert_to_one_hot(Y_train, C = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's Fit the Keras model on `X_train_indices` and `Y_train_oh`. We will use `epochs = 50` and `batch_size = 32`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "132/132 [==============================] - 2s 17ms/step - loss: 1.6085 - accuracy: 0.1818\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.5825 - accuracy: 0.3333\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.5681 - accuracy: 0.2576\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.5504 - accuracy: 0.3561\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.5343 - accuracy: 0.3258\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.5064 - accuracy: 0.4091\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.5003 - accuracy: 0.3712\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.4391 - accuracy: 0.4621\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.4020 - accuracy: 0.5227\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.3475 - accuracy: 0.6439\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.3148 - accuracy: 0.6212\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.2611 - accuracy: 0.6818\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.2579 - accuracy: 0.6894\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.2586 - accuracy: 0.6364\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.2063 - accuracy: 0.7500\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.2020 - accuracy: 0.7576\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.1548 - accuracy: 0.7727\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.1003 - accuracy: 0.8333\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.0855 - accuracy: 0.8258\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.0612 - accuracy: 0.8409\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.0457 - accuracy: 0.8939\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.0506 - accuracy: 0.8712\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.0597 - accuracy: 0.8561\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.0339 - accuracy: 0.8864\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.0127 - accuracy: 0.8939\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.0182 - accuracy: 0.8939\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.0446 - accuracy: 0.8636\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.0071 - accuracy: 0.9091\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.0536 - accuracy: 0.8485\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.0456 - accuracy: 0.8712\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.0746 - accuracy: 0.8333\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.0861 - accuracy: 0.8182\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.1298 - accuracy: 0.7652\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.1031 - accuracy: 0.8030\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.1406 - accuracy: 0.7576\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.0861 - accuracy: 0.8182\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.0466 - accuracy: 0.8636\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.0078 - accuracy: 0.9091\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.0063 - accuracy: 0.9091\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.9983 - accuracy: 0.9091\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.9859 - accuracy: 0.9242\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.0265 - accuracy: 0.8864\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.0074 - accuracy: 0.9015\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.9770 - accuracy: 0.9318\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.9888 - accuracy: 0.9167\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.9754 - accuracy: 0.9318\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.9684 - accuracy: 0.9394\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.9691 - accuracy: 0.9394\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.9767 - accuracy: 0.9318\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.9656 - accuracy: 0.9470\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x143393732e8>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_indices, Y_train_oh, epochs = 50, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model Accuracy on the training set is 93% now let's evaluate the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/56 [==============================] - 0s 8ms/step\n",
      "\n",
      "Test accuracy =  0.8214285969734192\n"
     ]
    }
   ],
   "source": [
    "X_test_indices = sentences_to_indices(X_test, word_to_index, max_len = maxLen)\n",
    "Y_test_oh = convert_to_one_hot(Y_test, C = 5)\n",
    "loss, acc = model.evaluate(X_test_indices, Y_test_oh)\n",
    "print()\n",
    "print(\"Test accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad on the test set but the model is overfitting so we can tune it further to reduce this overfitting and optimize the test set accuracy\n",
    "\n",
    "let's see some predictions for the model in the next cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not feeling happy üòû\n"
     ]
    }
   ],
   "source": [
    "# Change the sentence below to see your prediction. Make sure all the words are in the Glove embeddings.  \n",
    "x_test = np.array(['not feeling happy'])\n",
    "X_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\n",
    "print(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feeling good üòÑ\n"
     ]
    }
   ],
   "source": [
    "x_test = np.array(['feeling good'])\n",
    "X_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\n",
    "print(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love you ‚ù§Ô∏è\n"
     ]
    }
   ],
   "source": [
    "x_test = np.array(['i love you'])\n",
    "X_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\n",
    "print(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lets eat üç¥\n"
     ]
    }
   ],
   "source": [
    "x_test = np.array(['lets eat'])\n",
    "X_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\n",
    "print(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can further optimize and make our app more advanced if we train on larger dataset that contains much more emoji's "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
